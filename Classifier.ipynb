{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jules/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jules/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jules/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jules/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jules/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jules/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jules/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jules/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jules/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jules/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jules/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jules/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "\n",
    "print(tf.test.is_gpu_available())\n",
    "\n",
    "max_frame = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "\n",
    "https://pytorch.org/docs/stable/data.html\n",
    "\n",
    "https://github.com/kenshohara/3D-ResNets-PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_IDs = {}\n",
    "with open(\"data/dataset.json\", \"r\") as file:\n",
    "    list_IDs = json.load(file)\n",
    "labels = {}\n",
    "with open(\"data/labels.json\", \"r\") as file:\n",
    "    labels = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTUSequence(Sequence):\n",
    "    \n",
    "    def __init__(self, list_IDs, labels, path, batch_size=32, max_frame=max_frame, one_hot=True):\n",
    "        self.x = list_IDs\n",
    "        self.y = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.max_frame = max_frame\n",
    "        self.path = path\n",
    "        if one_hot:\n",
    "            self.one_hot_encode()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = np.array([self.y[ID] for ID in batch_x])\n",
    "        \n",
    "        X = [np.load(self.path + ID + '.npy')[:, :, :, np.newaxis] for ID in batch_x]\n",
    "        \n",
    "        if not(self.max_frame is None):\n",
    "            X = np.stack(\n",
    "                [x[:max_frame] if x.shape[0] >= max_frame else np.pad(x, ((0, max_frame-x.shape[0]),(0,0),(0,0),(0,0)), \"constant\") for x in X]\n",
    "            )     \n",
    "        \n",
    "        return X, batch_y\n",
    "\n",
    "    def one_hot_encode(self):\n",
    "        labels = [val for val in self.y.values()]\n",
    "        ids = [key for key in self.y.keys()]\n",
    "        labels = to_categorical(labels)\n",
    "        self.y = {ID: label for ID, label in zip(ids, labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = NTUSequence(list_IDs[\"train\"], labels, path=\"data/processed/train/\")\n",
    "testset = NTUSequence(list_IDs[\"validation\"], labels, path=\"data/processed/test/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv3D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_6 (Conv3D)            (None, 60, 25, 25, 64)    1792      \n",
      "_________________________________________________________________\n",
      "conv3d_7 (Conv3D)            (None, 60, 25, 25, 64)    110656    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 30, 12, 12, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_8 (Conv3D)            (None, 30, 12, 12, 128)   221312    \n",
      "_________________________________________________________________\n",
      "conv3d_9 (Conv3D)            (None, 30, 12, 12, 128)   442496    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 15, 6, 6, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 15, 6, 6, 256)     884992    \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 15, 6, 6, 256)     1769728   \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 7, 3, 3, 256)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16128)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 49)                790321    \n",
      "=================================================================\n",
      "Total params: 4,221,297\n",
      "Trainable params: 4,221,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv3d = models.Sequential([\n",
    "    layers.Conv3D(64, 3, activation=\"relu\", padding=\"same\", input_shape=(max_frame, 25, 25, 1)),\n",
    "    layers.Conv3D(64, 3, activation=\"relu\", padding=\"same\"),\n",
    "    layers.MaxPooling3D((2,2,2)),\n",
    "    \n",
    "    layers.Conv3D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "    layers.Conv3D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "    layers.MaxPooling3D((2,2,2)),\n",
    "    \n",
    "    layers.Conv3D(256, 3, activation=\"relu\", padding=\"same\"),\n",
    "    layers.Conv3D(256, 3, activation=\"relu\", padding=\"same\"),\n",
    "    layers.MaxPooling3D((2,2,2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    #layers.Dense(1, activation=\"relu\"),\n",
    "    layers.Dense(49, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "conv3d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(lr=1e-4, decay=1e-8)\n",
    "\n",
    "conv3d.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1157/1157 [==============================] - 286s 248ms/step - loss: 2.5749 - acc: 0.3008\n",
      "Epoch 2/20\n",
      "1157/1157 [==============================] - 282s 244ms/step - loss: 1.7453 - acc: 0.5063\n",
      "Epoch 3/20\n",
      "1157/1157 [==============================] - 282s 244ms/step - loss: 1.4254 - acc: 0.5892\n",
      "Epoch 4/20\n",
      "1157/1157 [==============================] - 283s 245ms/step - loss: 1.1998 - acc: 0.6472\n",
      "Epoch 5/20\n",
      "1157/1157 [==============================] - 284s 246ms/step - loss: 1.0096 - acc: 0.6978\n",
      "Epoch 6/20\n",
      "1157/1157 [==============================] - 287s 248ms/step - loss: 0.8337 - acc: 0.7469\n",
      "Epoch 7/20\n",
      "1157/1157 [==============================] - 283s 244ms/step - loss: 0.6740 - acc: 0.7901\n",
      "Epoch 8/20\n",
      "1157/1157 [==============================] - 287s 248ms/step - loss: 0.5336 - acc: 0.8303\n",
      "Epoch 9/20\n",
      "1157/1157 [==============================] - 288s 249ms/step - loss: 0.4134 - acc: 0.8688\n",
      "Epoch 10/20\n",
      "1157/1157 [==============================] - 290s 250ms/step - loss: 0.3264 - acc: 0.8928\n",
      "Epoch 11/20\n",
      "1157/1157 [==============================] - 290s 250ms/step - loss: 0.2515 - acc: 0.9171\n",
      "Epoch 12/20\n",
      "1157/1157 [==============================] - 290s 251ms/step - loss: 0.2138 - acc: 0.9280\n",
      "Epoch 13/20\n",
      "1157/1157 [==============================] - 289s 250ms/step - loss: 0.1862 - acc: 0.9373\n",
      "Epoch 14/20\n",
      "1157/1157 [==============================] - 289s 250ms/step - loss: 0.1528 - acc: 0.9482\n",
      "Epoch 15/20\n",
      "1157/1157 [==============================] - 288s 249ms/step - loss: 0.1327 - acc: 0.9546\n",
      "Epoch 16/20\n",
      "1157/1157 [==============================] - 283s 245ms/step - loss: 0.1235 - acc: 0.9601\n",
      "Epoch 17/20\n",
      "1157/1157 [==============================] - 284s 245ms/step - loss: 0.1128 - acc: 0.9613\n",
      "Epoch 18/20\n",
      "1157/1157 [==============================] - 291s 252ms/step - loss: 0.1049 - acc: 0.9648\n",
      "Epoch 19/20\n",
      "1157/1157 [==============================] - 291s 251ms/step - loss: 0.0968 - acc: 0.9677\n",
      "Epoch 20/20\n",
      "1157/1157 [==============================] - 290s 251ms/step - loss: 0.0899 - acc: 0.9700\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "history = conv3d.fit_generator(trainset, \n",
    "                     epochs=20,\n",
    "                    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = conv3d.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "conv3d.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jules/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "convnet = models.Sequential()\n",
    "\n",
    "convnet.add(layers.TimeDistributed(layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\", input_shape=(25, 25, 1))))\n",
    "convnet.add(layers.TimeDistributed(layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")))\n",
    "convnet.add(layers.TimeDistributed(layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")))\n",
    "convnet.add(layers.TimeDistributed(layers.MaxPooling2D((2,2))))\n",
    "\n",
    "convnet.add(layers.TimeDistributed(layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")))\n",
    "convnet.add(layers.TimeDistributed(layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")))\n",
    "convnet.add(layers.TimeDistributed(layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")))\n",
    "convnet.add(layers.TimeDistributed(layers.MaxPooling2D((2,2))))\n",
    "\n",
    "convnet.add(layers.TimeDistributed(layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\")))\n",
    "convnet.add(layers.TimeDistributed(layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\")))\n",
    "convnet.add(layers.TimeDistributed(layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\")))\n",
    "convnet.add(layers.TimeDistributed(layers.MaxPooling2D((2,2))))\n",
    "\n",
    "convnet.add(layers.TimeDistributed(layers.Flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = models.Sequential()\n",
    "lstm_model.add(layers.Input((None, 25, 25, 1)))\n",
    "lstm_model.add(convnet)\n",
    "lstm_model.add(layers.LSTM(5))\n",
    "lstm_model.add(layers.Dense(49))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      (None, None, 2304)        1918848   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 5)                 46200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 49)                294       \n",
      "=================================================================\n",
      "Total params: 1,965,342\n",
      "Trainable params: 1,965,342\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(lr=1e-4, decay=1e-8)\n",
    "lstm_model.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "log_dir=\"logs/LSTM/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "history = lstm_model.fit_generator(trainset, \n",
    "                     epochs=20,\n",
    "                   callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jules/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "convnet = models.Sequential([\n",
    "    \n",
    "    layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\", input_shape=(25, 25, 1)),\n",
    "    layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\"),\n",
    "    layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\"),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    \n",
    "    layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "    layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    \n",
    "    layers.Flatten()\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 25, 25, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 25, 25, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 25, 25, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "=================================================================\n",
      "Total params: 295,936\n",
      "Trainable params: 295,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "convnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Laurent_Simon_Tes_Morts = models.Sequential([\n",
    "    layers.Input((None, 25, 25, 1)),\n",
    "    layers.TimeDistributed(convnet),\n",
    "    \n",
    "    layers.LSTM(5),\n",
    "    \n",
    "    layers.Dense(49)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Laurent_Simon_Tes_Morts.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "\n",
    "log_dir=\"logs\\\\LSTM\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "print(log_dir)\n",
    "\n",
    "history = Laurent_Simon_Tes_Morts.fit_generator(trainset, \n",
    "                     epochs=20,\n",
    "                   callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTUDataset(data.Dataset):\n",
    "    def __init__(self, list_IDs, labels, path, max_frame=max_frame):\n",
    "            self.max_frame = max_frame\n",
    "            self.path = path\n",
    "            self.labels = labels\n",
    "            self.list_IDs = list_IDs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "        array = np.load(self.path + ID + '.npy')\n",
    "        if not(self.max_frame is None):\n",
    "            mid_frame = array.shape[0] // 2\n",
    "            array = array[mid_frame-self.max_frame//2:mid_frame+self.max_frame//2]\n",
    "        # Load data and get label\n",
    "        X = torch.from_numpy(array)\n",
    "        y = self.labels[ID]\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "trainset = NTUDataset(list_IDs[\"train\"], labels, path=\"data/processed/train/\")\n",
    "testset = NTUDataset(list_IDs[\"validation\"], labels, path=\"data/processed/test/\")\n",
    "\n",
    "train_gen = data.DataLoader(trainset, **dataloader_config)\n",
    "test_gen = data.DataLoader(testset, **dataloader_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SpatialClassifier, self).__init__()\n",
    "        self.conv11 = nn.Conv3d(max_frame, 64, 3)\n",
    "        self.conv12 = nn.Conv3d(64, 64, 3)\n",
    "        \n",
    "        self.conv21 = nn.Conv3d(128, 128, 3)\n",
    "        self.conv22 = nn.Conv3d(128, 128, 3)\n",
    "\n",
    "        self.conv31 = nn.Conv3d(256, 256, 3)\n",
    "        self.conv32 = nn.Conv3d(256, 256, 3)\n",
    "        \n",
    "        self.fc = nn.Linear(256*6*6, 11)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv11(x))\n",
    "        x = F.relu(self.conv22(x))\n",
    "        x = F.max_pool3d(2)\n",
    "        \n",
    "        x = F.relu(self.conv21(x))\n",
    "        x = F.relu(self.conv22(x))\n",
    "        x = F.max_pool3d(2)\n",
    "        \n",
    "        x = F.relu(self.conv31(x))\n",
    "        x = F.relu(self.conv32(x))\n",
    "        x = F.max_pool3d(2)\n",
    "        \n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SpatialClassifier()\n",
    "net.to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = 0.0\n",
    "for i, data in enumerate(train_gen, 0):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs, labels = data\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "    if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "        print('[%d, %5d] loss: %.3f' %\n",
    "              (epoch + 1, i + 1, running_loss / 2000))\n",
    "        running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
