{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jules/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jules/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jules/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jules/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jules/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jules/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jules/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jules/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jules/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jules/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jules/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jules/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime \n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "\n",
    "print(tf.test.is_gpu_available())\n",
    "\n",
    "min_frame = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_IDs = {}\n",
    "with open(\"data/dataset.json\", \"r\") as file:\n",
    "    list_IDs = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTUSequence(Sequence):\n",
    "    \n",
    "    def __init__(self, list_IDs, path, batch_size=32, one_hot=True, min_frame=min_frame):\n",
    "        # Removed unused vars (self.x, max_frame)\n",
    "        self.x = list_IDs\n",
    "        self.batch_size = batch_size\n",
    "        self.path = path\n",
    "        self.min_frame = min_frame\n",
    "        if one_hot:\n",
    "            self.one_hot_encode()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # idx: index of the data\n",
    "    \n",
    "        # Should be the same as normal\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        \n",
    "        #list of batch_x files \n",
    "        F = [np.load(self.path + ID + '.npy')[:, :, :, np.newaxis] for ID in batch_x]\n",
    "        \n",
    "        if not(self.min_frame is None):\n",
    "            X, Y = [], []\n",
    "            \n",
    "            for x in F:\n",
    "                if min_frame >= x.shape[0] - 1:\n",
    "                \n",
    "                    X.append(min_frame * [x[0]])\n",
    "                    Y.append(x[0])\n",
    "                else :\n",
    "                    f0 = random.randint(min_frame, x.shape[0] - 1)\n",
    "                    X.append(x[f0 - min_frame : f0])\n",
    "                    Y.append(x[f0])\n",
    "                \n",
    "            X = np.stack(X)\n",
    "            Y = np.stack(Y)\n",
    "#         print(\"X=\")\n",
    "#         print(X.shape)\n",
    "#         print(\"Y=\")\n",
    "#         print(Y.shape)\n",
    "        \n",
    "        return X, Y\n",
    "\n",
    "    def one_hot_encode(self):\n",
    "#         labels = [val for val in self.y.values()]\n",
    "#         ids = [key for key in self.y.keys()]\n",
    "#         labels = to_categorical(labels)\n",
    "#         self.y = {ID: label for ID, label in zip(ids, labels)}\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = NTUSequence(list_IDs[\"train\"], path=\"data/processed/train/\")\n",
    "testset = NTUSequence(list_IDs[\"validation\"], path=\"data/processed/test/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "convnet = models.Sequential([\n",
    "    \n",
    "    layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\", input_shape=(25, 25, 1)),\n",
    "    layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\"),\n",
    "    layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\"),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    \n",
    "    layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "    layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    \n",
    "    layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
    "    layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "                        \n",
    "    layers.Conv2D(512, 3, activation=\"relu\", padding=\"same\"),\n",
    "    layers.Conv2D(512, 3, activation=\"relu\", padding=\"same\"),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    \n",
    "    layers.Flatten()\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 25, 25, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 25, 25, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 25, 25, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 6, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 3, 3, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 4,721,152\n",
      "Trainable params: 4,721,152\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "convnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = models.Sequential([\n",
    "    layers.Input((None, 25, 25, 1)),\n",
    "    layers.TimeDistributed(convnet),\n",
    "    \n",
    "    layers.LSTM(9, activation=\"relu\"),\n",
    "    layers.Reshape((3,3,1)),\n",
    "    \n",
    "    layers.Conv2DTranspose(1, 3, strides=2, activation=\"relu\", padding=\"same\"),\n",
    "    layers.Conv2DTranspose(1, 3, strides=2, activation=\"relu\", padding=\"same\"),\n",
    "    layers.Conv2DTranspose(1, 3, strides=2, activation=\"sigmoid\"),\n",
    "    \n",
    "    layers.Reshape((25,25,1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_1 (TimeDist (None, None, 512)         4721152   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 9)                 18792     \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 3, 3, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 6, 6, 1)           10        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 12, 12, 1)         10        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 25, 25, 1)         10        \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 25, 25, 1)         0         \n",
      "=================================================================\n",
      "Total params: 4,739,974\n",
      "Trainable params: 4,739,974\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "predictor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.compile(loss=\"mean_squared_logarithmic_error\", optimizer=\"adam\", metrics=[\"mse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/predictor/fit/20200130-083334\n",
      "Epoch 1/5\n",
      "1157/1157 [==============================] - 30s 26ms/step - loss: 0.0259 - mean_squared_error: 0.0517\n",
      "Epoch 2/5\n",
      "1157/1157 [==============================] - 30s 26ms/step - loss: 0.0258 - mean_squared_error: 0.0515\n",
      "Epoch 3/5\n",
      "1157/1157 [==============================] - 31s 26ms/step - loss: 0.0258 - mean_squared_error: 0.0515\n",
      "Epoch 4/5\n",
      "1157/1157 [==============================] - 31s 26ms/step - loss: 0.0257 - mean_squared_error: 0.0514\n",
      "Epoch 5/5\n",
      "1157/1157 [==============================] - 31s 27ms/step - loss: 0.0257 - mean_squared_error: 0.0513\n"
     ]
    }
   ],
   "source": [
    "log_dir=\"logs/predictor/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "print(log_dir)\n",
    "\n",
    "history = predictor.fit_generator(trainset, \n",
    "                     epochs=5,\n",
    "                   callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
