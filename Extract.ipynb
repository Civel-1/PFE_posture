{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Insert path to .skeleton files here\n",
    "path = 'data/videos/nturgb+d_skeletons/'\n",
    "train_dest_path = 'data/processed/train/'\n",
    "test_dest_path = 'data/processed/test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction et conversion des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix(x, y, z, matrix=None, reduce=True):\n",
    "    \"\"\"\n",
    "    Compute distance matrix.\n",
    "    \n",
    "    Args:\n",
    "        x, y, z: coordonates of points\n",
    "         matrix: matrix to update, if already built\n",
    "         reduce: reduce matrix coeffs in [0, 1]\n",
    "     \n",
    "    Return:\n",
    "        matrix: distance matrix\n",
    "    \"\"\"\n",
    "    assert x.size == y.size == z.size\n",
    "    nb_pt = x.size\n",
    "    if matrix is None:\n",
    "        matrix = np.zeros((25, 25), dtype=\"float32\")\n",
    "    \n",
    "    for i in range(nb_pt):\n",
    "        for j in range(nb_pt):\n",
    "            matrix[i, j] = math.sqrt((x[i] - x[j])**2 + (y[i] - y[j])**2 + (z[i] - z[j])**2)\n",
    "            \n",
    "    if reduce:\n",
    "        matrix = (matrix - matrix.min()) / matrix.max()\n",
    "        \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Training Set\n",
      "144 corrupted files.7161/37161 - - - Percentage Complete = 100.00%\n",
      "Final training data dimensions: (0,)\n",
      "One-hot classes matrix:\n",
      " [ 0  1  2 ... 16 17 18]\n",
      "Final training labels dimensions: (37161,) \n",
      "\n",
      "Processing Test Set\n",
      "13 corrupted files.9291/9291 - - - Percentage Complete = 100.00%\n",
      "Final Training data dimensions: (485,)\n",
      "One-hot classes matrix:\n",
      " [19 20 21 ... 46 47 48]\n",
      "Final test labels dimensions: (9291,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Author: Henry Powell\n",
    "Institution: Institute of Neuroscience and Psychology, Glasgow University, Scotland.\n",
    "\n",
    "Python script for formatting the NTU RGB+D Skeletons data set into a format suitable for most LSTM RNNs. The aim is to\n",
    "take each .skeletons file and compress it into a 3D numpy array with [samples, time-steps, features] as its dimensions.\n",
    "The final data set will thus be a [56,881, max(len(samples(data_files)))=600, 12*25=300] numpy array. The data has\n",
    "been left normal (i.e. not normalized) for the sake of flexibility although it is generally recommended to normalize\n",
    "the data at some stage in the preprocessing.\n",
    "\"\"\"\n",
    "\n",
    "# Keep track of total files processed\n",
    "total_files = 0\n",
    "\n",
    "# List of class numbers labels from data_set\n",
    "NTU_classes = [c for c in range(1, 49)]\n",
    "\n",
    "\n",
    "def filter_missing_samples():\n",
    "    \"\"\" Function to filter out all of the samples from the data set that have no data in them.\n",
    "\n",
    "        Returns: list object containing str(filenames) of all files with no data\n",
    "\n",
    "    \"\"\"\n",
    "    # List of files with missing data.\n",
    "    missing = np.load(\"data/utils/missing_samples.npy\")\n",
    "\n",
    "    missing_skeleton = [path + i + '.skeleton' for i in missing]\n",
    "    missing = missing_skeleton\n",
    "    del missing_skeleton\n",
    "    gc.collect()\n",
    "    return missing\n",
    "\n",
    "\n",
    "def load_files(path, missing, fix_total_files=False, prop_files=100, batch_type='train', drop_first=False):\n",
    "\n",
    "    \"\"\"\n",
    "    :param path: Path to the data set.\n",
    "    :param missing: List of files with no data.\n",
    "    :param fix_total_files: Specify the number of data files you want to process up to 56,881\n",
    "    :param prop_files: What proportion of the fix_total_files you want to load.\n",
    "    :param batch_type: Splits the loaded files into either 80% of total files if batch_type = 'train', or 20% of\n",
    "                       total files if batch_type = 'test'.\n",
    "    :param drop_first: Stop function from iterating over .CD file if there is one present in the directory.\n",
    "    :return: List of .skeleton files as posixpath objects\n",
    "    \"\"\"\n",
    "\n",
    "    directory = Path(path)\n",
    "    \n",
    "    # Store files as list to be iterated through\n",
    "    # Suppression des classes avec interaction de squelettes\n",
    "    files = [p for p in directory.iterdir() if p.is_file() and str(p) not in missing and not(int(str(p)[48:51]) > 49)] \n",
    "\n",
    "    # You may have a .CD file hidden in this folder. This drops this from [files] so that the code doesn't run over it.\n",
    "    if drop_first:\n",
    "        files.pop(0)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if fix_total_files:\n",
    "        files = files[:fix_total_files]\n",
    "    else:\n",
    "        files = files\n",
    "\n",
    "    # Number of total files before dropping if files_batch_prop < 100\n",
    "    total_num_files = len(files)\n",
    "    file_percentage = (total_num_files / 100) * prop_files\n",
    "\n",
    "    # Drop proportion of files you don't want to process\n",
    "    if prop_files == 100:\n",
    "        files = files\n",
    "    elif prop_files != 100 and batch_type == 'train':\n",
    "        files = files[:int(file_percentage)]\n",
    "    elif prop_files != 100 and batch_type == 'test':\n",
    "        files = files[int(file_percentage):]\n",
    "    elif prop_files > 100 or prop_files < 0:\n",
    "        raise Exception('files_batch_prop should be an integer between 0 and 100. You gave {}'.format(prop_files))\n",
    "    gc.collect()\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def get_classes(files, one_hot=False, subset=False):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    :param files: list of .skeleton files to be processed (must be posixPath object)\n",
    "    :param one_hot: translate classes to a one-hot encoding\n",
    "    :param subset: specify that you are using a the binary subset of the dataset (Action 1 and Action 3 (A001 & A003)\n",
    "    :return: list of classes\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    files = [str(f) for f in files]\n",
    "    class_list = list()\n",
    "    class_index = files[0].find('A0')\n",
    "\n",
    "    for i in range(len(files)):\n",
    "        class_list.append(files[i][class_index+2:class_index+4])\n",
    "    del class_index\n",
    "\n",
    "    class_list = [int(c)-1 for c in class_list]\n",
    "    class_list = np.array(class_list)\n",
    "\n",
    "    if one_hot:\n",
    "        # One-hot encode integers to make suitable for LSTM\n",
    "        class_list = keras.utils.to_categorical(class_list)\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    gc.collect()\n",
    "    return class_list\n",
    "\n",
    "\n",
    "def process_raw_data(files, dest_path, save_as_ndarray=False, three_d=True, derivative=False):\n",
    "\n",
    "    \"\"\"\n",
    "    :param files: list of .skeleton files to be processed (must be posixPath object)\n",
    "    :param save_as_ndarray: set to True to save the outputted data to an ndarray in the current directory\n",
    "    :param derivative: add feature engineered columns to the output. Adds first derivative calculations to each\n",
    "                       position point in x,y,z dimensions.\n",
    "    :param three_d: set to False if you only want the three d position features for each time frame\n",
    "    :return: np.array of dimension (samples, time_steps, features)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # This variable tracks how many files have been formatted and added to the new data set\n",
    "    progress = 0\n",
    "    file_nb = len(files)\n",
    "    loaded = list()\n",
    "    errors = list()\n",
    "\n",
    "    # Iteration loop which formats the .skeleton files.\n",
    "    for file in files:\n",
    "        if not(os.path.isfile(dest_path + str(file)[31:51] + \".npy\")):\n",
    "            try:\n",
    "                features = list()\n",
    "                row = list()\n",
    "\n",
    "                data = pd.read_csv(file, header=None)\n",
    "                # on ne conserve que les données dont la taille démontre qu'il s'agit de nombres flottants\n",
    "                data['length'] = data[0].apply(lambda x: len(str(x)))\n",
    "                cond = data['length'] > 10\n",
    "                data = data[cond]\n",
    "                # ré-indexage\n",
    "                data = data.reset_index(drop=True)\n",
    "                # suppression des en-têtes de frame\n",
    "                data = data[data.index % 26 != 0]\n",
    "                # supression de la column de tri par la longueur\n",
    "                data = data.drop(columns=['length'])\n",
    "                # ré-indexage\n",
    "                data = data.reset_index(drop=True)\n",
    "                # formattage des données numériques\n",
    "                data = data[0].str.split(\" \", expand=True)\n",
    "                data = data.fillna(method='bfill')\n",
    "                if three_d:\n",
    "                    data = data.drop(columns=[3, 4, 5, 6, 7, 8, 9, 10, 11])\n",
    "                # noms des variables\n",
    "                data.columns = [\"x\", \"y\", \"z\"]\n",
    "\n",
    "                X = np.array(data.x, dtype=\"float32\")\n",
    "                Y = np.array(data.y, dtype=\"float32\")\n",
    "                Z = np.array(data.z, dtype=\"float32\")\n",
    "\n",
    "                frames = len(data.index) // 25\n",
    "                for i in range(frames):\n",
    "                    x = X[i*25:(i+1)*25]\n",
    "                    y = Y[i*25:(i+1)*25]\n",
    "                    z = Z[i*25:(i+1)*25]\n",
    "                    features.append(distance_matrix(x, y, z))\n",
    "\n",
    "                del data\n",
    "                gc.collect()\n",
    "\n",
    "                features = np.array(features)\n",
    "\n",
    "                loaded.append(features)\n",
    "\n",
    "                if save_as_ndarray:\n",
    "                    np.save(os.path.join(dest_path, str(file)[31:51]), features)\n",
    "\n",
    "                # Sanity check to ensure all the matrices are of the right dimension (Uncomment the below to make check)\n",
    "                # print(features.shape)\n",
    "\n",
    "            except:\n",
    "                errors.append(file)\n",
    "            \n",
    "        progress += 1\n",
    "        perc = progress/(file_nb/100)\n",
    "        print(f'Samples Processed: {progress}/{file_nb} - - - Percentage Complete = {perc:.2f}%', end='\\r')\n",
    "\n",
    "        if progress == total_files:\n",
    "            print(f'Samples Processed: {progress}/{file_nb} - - - Percentage Complete = {100}%')\n",
    "            \n",
    "    loaded = np.array(loaded)\n",
    "\n",
    "    return loaded, errors\n",
    "\n",
    "\n",
    "def preprocess_training(training_split_size=80, fix_total_files=1000, sanity=False, save=False, one_hot=False):\n",
    "\n",
    "    print('Processing Training Set')\n",
    "\n",
    "    missing = filter_missing_samples()\n",
    "    files = load_files(path, missing, prop_files=training_split_size,\n",
    "                       batch_type='train', fix_total_files=fix_total_files)\n",
    "    classes = get_classes(files, one_hot=one_hot)\n",
    "    \n",
    "    mapping = [np.array([str(f)[31:51], c]) for f, c in zip(files, classes)]\n",
    "    pd.DataFrame(mapping, columns=[\"file\", \"label\"]).to_csv(\"data/utils/trainset.csv\")\n",
    "    \n",
    "    loaded, errors = process_raw_data(files, train_dest_path, save_as_ndarray=True)\n",
    "    \n",
    "    print(f\"{len(errors)} corrupted files.\")\n",
    "    with open(\"data/utils/train_errors.json\", \"w+\") as file:\n",
    "        json.dump([str(e) for e in errors], file)\n",
    "\n",
    "    if save:\n",
    "        np.save('skeletons_array_train_S', loaded)\n",
    "        np.save('skeletons_array_train_labels_S', classes)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Sanity check to ensure resulting matrix is of the right shape\n",
    "    print('Final training data dimensions: {}'.format(loaded.shape))\n",
    "\n",
    "    if sanity:\n",
    "        print('One-hot classes matrix:\\n', classes)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print('Final training labels dimensions: {} \\n'.format(classes.shape))\n",
    "\n",
    "    return loaded, classes\n",
    "\n",
    "\n",
    "def preprocess_test(training_split_size=80, fix_total_files=1000, sanity=False, save=True):\n",
    "\n",
    "    print('Processing Test Set')\n",
    "    missing = filter_missing_samples()\n",
    "    files = load_files(path, missing, prop_files=training_split_size,\n",
    "                       batch_type='test', fix_total_files=fix_total_files)\n",
    "    classes = get_classes(files)\n",
    "    \n",
    "    mapping = [np.array([str(f)[31:51], c]) for f, c in zip(files, classes)]\n",
    "    pd.DataFrame(mapping, columns=[\"file\", \"label\"]).to_csv(\"data/utils/testset.csv\")\n",
    "    \n",
    "    loaded, errors = process_raw_data(files, test_dest_path, save_as_ndarray=True)\n",
    "    \n",
    "    print(f\"{len(errors)} corrupted files.\")\n",
    "    with open(\"data/utils/test_errors.json\", \"w+\") as file:\n",
    "        json.dump([str(e) for e in errors], file)\n",
    "\n",
    "    if save:\n",
    "        np.save('skeletons_array_test_S', loaded)\n",
    "        np.save('skeletons_array_test_labels_S', classes)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Sanity check to ensure resulting matrix is of the right shape\n",
    "    print('Final Training data dimensions: {}'.format(loaded.shape))\n",
    "\n",
    "    if sanity:\n",
    "        print('One-hot classes matrix:\\n', classes)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print('Final test labels dimensions: {} \\n'.format(classes.shape))\n",
    "\n",
    "    return loaded, classes\n",
    "\n",
    "\n",
    "def get_test_train(training_split_size=80, fix_total_files=60, sanity=False, save=True):\n",
    "\n",
    "    preprocess_training(training_split_size=training_split_size, fix_total_files=fix_total_files, sanity=sanity, save=save)\n",
    "    preprocess_test(training_split_size=training_split_size, fix_total_files=fix_total_files, sanity=sanity, save=save)\n",
    "\n",
    "\n",
    "get_test_train(training_split_size=80, fix_total_files=False, sanity=True, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage des données\n",
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = Path(train_dest_path)\n",
    "train_files = [str(p)[21:41] for p in train_dir.iterdir() if p.is_file()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37161"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = pd.read_csv(\"data/utils/trainset.csv\", sep=\",\")\n",
    "trainset.columns = [\"index\", \"file\", \"label\"]\n",
    "trainset.drop(columns=[\"index\"], inplace=True)\n",
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37161it [00:11, 3314.04it/s]\n"
     ]
    }
   ],
   "source": [
    "filtered_train = trainset.copy()\n",
    "for i, file in tqdm(enumerate(trainset.file)):\n",
    "    if not(file in train_files):\n",
    "        filtered_train.drop([i], axis=0, inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37017"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>S001C001P001R001A001</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>S001C001P001R001A002</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>S001C001P001R001A003</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>S001C001P001R001A004</td>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>S001C001P001R001A005</td>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   file  label    set\n",
       "0  S001C001P001R001A001      0  train\n",
       "1  S001C001P001R001A002      1  train\n",
       "2  S001C001P001R001A003      2  train\n",
       "3  S001C001P001R001A004      3  train\n",
       "4  S001C001P001R001A005      4  train"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_train[\"set\"] = [\"train\" for _ in range(len(filtered_train))]\n",
    "filtered_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = Path(test_dest_path)\n",
    "test_files = [str(p)[20:40] for p in test_dir.iterdir() if p.is_file()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9291"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset = pd.read_csv(\"data/utils/testset.csv\", sep=\",\")\n",
    "testset.columns = [\"index\", \"file\", \"label\"]\n",
    "testset.drop(columns=[\"index\"], inplace=True)\n",
    "len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9291it [00:00, 13645.44it/s]\n"
     ]
    }
   ],
   "source": [
    "filtered_test = testset.copy()\n",
    "for i, file in tqdm(enumerate(testset.file)):\n",
    "    if not(file in test_files):\n",
    "        filtered_test.drop([i], axis=0, inplace=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9278"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>S014C001P008R001A020</td>\n",
       "      <td>19</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>S014C001P008R001A021</td>\n",
       "      <td>20</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>S014C001P008R001A022</td>\n",
       "      <td>21</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>S014C001P008R001A023</td>\n",
       "      <td>22</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>S014C001P008R001A024</td>\n",
       "      <td>23</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   file  label         set\n",
       "0  S014C001P008R001A020     19  validation\n",
       "1  S014C001P008R001A021     20  validation\n",
       "2  S014C001P008R001A022     21  validation\n",
       "3  S014C001P008R001A023     22  validation\n",
       "4  S014C001P008R001A024     23  validation"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_test[\"set\"] = [\"validation\" for _ in range(len(filtered_test))]\n",
    "filtered_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    \"train\": [f for f in filtered_train.file],\n",
    "    \"validation\": [f for f in filtered_test.file]\n",
    "}\n",
    "\n",
    "concat = pd.concat([filtered_train, filtered_test], axis=0)\n",
    "\n",
    "labels = {f: l for f, l in zip(concat.file, concat.label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset.json\", \"w+\") as file:\n",
    "    json.dump(dataset, file)\n",
    "with open(\"labels.json\", \"w+\") as file:\n",
    "    json.dump(labels, file)\n",
    "\n",
    "stats = {\n",
    "    \"train_len\": len(filtered_train),\n",
    "    \"test_len\": len(filtered_test)\n",
    "}\n",
    "\n",
    "with open(\"datastats.json\", \"w+\") as file:\n",
    "    json.dump(stats, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
